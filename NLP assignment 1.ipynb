{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb46703",
   "metadata": {},
   "source": [
    "## 1. Explain One-Hot Encoding ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e793b2cd",
   "metadata": {},
   "source": [
    "One-hot encoding is a technique used to convert categorical data into numerical data that can be processed by machine learning models. In one-hot encoding, a categorical feature is represented as a vector of binary values, where each value corresponds to a unique category in the feature. The length of the vector is equal to the number of unique categories in the feature, and only one value is 1 (hot) and the rest are 0 (cold). This means that each category is represented by a unique vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b350f49",
   "metadata": {},
   "source": [
    "## 2. Explain Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc2cef2",
   "metadata": {},
   "source": [
    "Bag of Words is a method used to represent text data as a vector of word frequencies. In this method, the text is first tokenized into individual words, and then a vocabulary is created consisting of all unique words in the text. Each word in the vocabulary is assigned a unique index, and a vector is created for each document in the corpus, where each element in the vector represents the frequency of the corresponding word in the document. The vector representation of a document is called the Bag of Words representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cde398",
   "metadata": {},
   "source": [
    "## 3. Explain Bag of N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8974e",
   "metadata": {},
   "source": [
    "Bag of N-Grams is a variation of the Bag of Words method, where instead of individual words, N-grams (contiguous sequences of N words) are used as features. The Bag of N-Grams method is more powerful than the Bag of Words method because it can capture the context of words, but it is also more computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6b5f6d",
   "metadata": {},
   "source": [
    "## 4. Explain TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7122b3f1",
   "metadata": {},
   "source": [
    "TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a method used to weight the importance of words in a document. In TF-IDF, the frequency of each word in a document is multiplied by the inverse frequency of the word in the corpus. The idea behind this is that words that appear frequently in a document but infrequently in the corpus are more important for that document. The TF-IDF score for a word is calculated as follows:\n",
    "\n",
    "TF-IDF(w, d) = TF(w, d) * IDF(w)\n",
    "where TF(w, d) is the frequency of word w in document d, and IDF(w) is the inverse document frequency of word w."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3d8ccf",
   "metadata": {},
   "source": [
    "## 5. What is OOV problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8fc9a4",
   "metadata": {},
   "source": [
    "Out-Of-Vocabulary (OOV) words is an important problem in NLP, we will introduce how to process words that are out of vocabulary in this tutorial.\n",
    "\n",
    "We often use word2vec or glove to process documents to create word vector or word embedding.\n",
    "However, we may ignore some words that appear rarely in documents, which may cause OOV problem.\n",
    "Meanwhile, we may use some pre-trained word representation file, which may do not contain some words in our data set. It also can cause OOV problem.\n",
    "\n",
    "How to fix OOV problem?\n",
    "There are three main ways that often be used in AI application.\n",
    "\n",
    "Ingoring them\n",
    "Generally, words that are out of vocabulary often appear rarely, the will contribute less to our model. The performance of our model will drop scarcely, it means we can ignore them.\n",
    "\n",
    "Replacing them using <UNK>\n",
    "We can replace all words that are out of vocabulary by using word <UNK>.\n",
    "    \n",
    "Initializing them by a uniform distribution with range [-0.01, 0.01]\n",
    "Out-Of-Vocabulary (OOV) words can be initialized from a uniform distribution with range [-0.01, 0.01]. We can use this uniform distribution to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e8b87",
   "metadata": {},
   "source": [
    "## 6. What are word embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f5096",
   "metadata": {},
   "source": [
    "Word embeddings are a family of natural language processing techniques used to represent words as numerical vectors. They are learned from large amounts of text data using unsupervised learning algorithms such as Word2Vec, GloVe, and FastText. Word embeddings aim to capture the meaning of words and their relationships with other words in a given language. They are used in a wide range of natural language processing tasks such as text classification, sentiment analysis, and machine translation.\n",
    "\n",
    "Word embeddings typically represent each word as a high-dimensional vector, with each dimension representing a different aspect of the word's meaning. For example, the word \"king\" might be represented as a vector with dimensions such as \"royalty\", \"power\", and \"authority\". These vectors are learned in such a way that words with similar meanings are mapped to similar regions in the vector space. This means that word embeddings can be used to capture semantic relationships between words, such as synonyms, antonyms, and analogies.\n",
    "\n",
    "Word embeddings have become an important tool in natural language processing because they can provide a way to represent text data as numerical vectors that can be processed by machine learning models. They have been shown to improve the accuracy of many natural language processing tasks, and they have become a standard technique in the field.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3167d2",
   "metadata": {},
   "source": [
    "## 7. Explain Continuous bag of words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e7cd79",
   "metadata": {},
   "source": [
    "Continuous Bag of Words (CBOW) is a natural language processing technique used for generating word embeddings. CBOW is a neural network architecture that learns to predict a target word given the context of the surrounding words. It is called \"continuous\" because it learns a continuous representation of the input words.\n",
    "\n",
    "The CBOW model takes as input a sequence of words and tries to predict the target word based on the surrounding context words. The context words are represented as one-hot vectors, and the model tries to predict the target word by learning a weight matrix that maps the context words to the target word. The weight matrix can be seen as a lookup table that maps the one-hot vector representations of the context words to a dense vector representation of the target word.\n",
    "\n",
    "During training, the CBOW model learns to adjust the weights of the lookup table in such a way that it can predict the target word based on the context words. The model learns to minimize the cross-entropy loss between the predicted probability distribution over the vocabulary and the actual probability distribution of the target word.\n",
    "\n",
    "Once trained, the CBOW model can be used to generate word embeddings by taking the learned weight matrix as the embedding matrix. Each row of the matrix corresponds to a word in the vocabulary, and the row vector represents the learned embedding for that word.\n",
    "\n",
    "CBOW is a popular technique for generating word embeddings because it is computationally efficient and has been shown to perform well on a wide range of natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecabeb7b",
   "metadata": {},
   "source": [
    "## 8. Explain SkipGram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9fc36e",
   "metadata": {},
   "source": [
    "SkipGram is another neural network architecture used for generating word embeddings. Unlike CBOW, which tries to predict the target word given the surrounding context words, SkipGram tries to predict the surrounding context words given the target word.\n",
    "\n",
    "The SkipGram model takes as input a target word and tries to predict the context words that are likely to appear in its surrounding context. Similar to CBOW, the model learns a weight matrix that maps the one-hot vector representation of the target word to a dense vector representation. During training, the model learns to adjust the weights of the lookup table in such a way that it can predict the context words based on the target word. The model learns to minimize the cross-entropy loss between the predicted probability distribution over the context words and the actual probability distribution of the context words.\n",
    "\n",
    "Once trained, the SkipGram model can be used to generate word embeddings by taking the learned weight matrix as the embedding matrix. Each row of the matrix corresponds to a word in the vocabulary, and the row vector represents the learned embedding for that word.\n",
    "\n",
    "SkipGram is a popular technique for generating word embeddings because it can capture more complex relationships between words than CBOW. It has been shown to perform well on tasks such as word similarity and analogy tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f472d7fc",
   "metadata": {},
   "source": [
    "## 9. Explain Glove Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06eb04",
   "metadata": {},
   "source": [
    "GloVe (Global Vectors for Word Representation) is a word embedding model that uses a co-occurrence matrix to generate word embeddings. The co-occurrence matrix measures the frequency of how often each word appears in the context of every other word in a large corpus of text. The rows of the matrix correspond to the context words, and the columns correspond to the target words.\n",
    "\n",
    "The GloVe model aims to learn a set of word embeddings that capture the relationship between the words in the co-occurrence matrix. The model does this by optimizing a loss function that tries to minimize the difference between the dot product of two word embeddings and the log of the co-occurrence count of the corresponding words.\n",
    "\n",
    "GloVe embeddings have been shown to perform well on a wide range of natural language processing tasks, including text classification, sentiment analysis, and machine translation. They are particularly useful for capturing relationships between words that are not captured by other word embedding models, such as synonyms, antonyms, and analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d0e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
